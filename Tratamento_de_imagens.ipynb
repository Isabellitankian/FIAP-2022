{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Isabellitankian/FIAP-2022/blob/main/Tratamento_de_imagens.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwmkD9bLjHQY"
      },
      "source": [
        "**Checkpoint - Classifica√ß√£o de Imagens com CNNs**  ‚òÅ\n",
        "\n",
        "**Isabell√≠ Andrade** - RM: 96078 ‚öõ\n",
        "\n",
        "**Caio Santos** - RM: 93250\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p9Vk37qjcxR"
      },
      "source": [
        "Come√ßar importando os cavaleiros do apocalipse: üê¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPPFyFPsjCd8"
      },
      "outputs": [],
      "source": [
        "import matplotlib as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns \n",
        "import tensorflow as tf \n",
        "import keras \n",
        "from keras.utils.np_utils import to_categorical\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import Sequential, layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from glob import glob\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLBQjNdoYs8A"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6BoA2mbogcZ"
      },
      "source": [
        "Importando os dados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "HHjjbtT3kVjK",
        "outputId": "ef87def4-3ece-4e14-8b0c-2186cd843d31"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bfedf1d9-2a45-420c-8222-147586c9ce1e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bfedf1d9-2a45-420c-8222-147586c9ce1e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeJP5nQ6oijy"
      },
      "source": [
        "Baixando o dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFWuCfQvYrC3"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d paultimothymooney/breast-histopathology-images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWOqYghP2spC"
      },
      "source": [
        "Crie uma pasta data e descompacte o dataset nela."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDxhmXEC2rRC"
      },
      "outputs": [],
      "source": [
        "!mkdir data\n",
        "!unzip breast-histopathology-images.zip -d data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu3sQvhAp5JK"
      },
      "source": [
        "## Preparando os dados para uso\n",
        "\n",
        "Descubra os nomes das imagens da classe 0 (negativa, n√£o c√¢ncer) e da classe 1 (positiva, c√¢ncer). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sTT3DzTp61m"
      },
      "outputs": [],
      "source": [
        "imagePatches = glob(\"/content/data/IDC_regular_ps50_idx5/**/*.png\", recursive=True)\n",
        "\n",
        "class0 = []\n",
        "class1 = []\n",
        "\n",
        "for filename in imagePatches:\n",
        "  if filename.endswith(\"class0.png\"):\n",
        "    class0.append(filename)\n",
        "  else:\n",
        "    class1.append(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zCgpub_p_tH"
      },
      "source": [
        "Diminua o tamanho do dataset, pois n√£o queremos demorar demais no treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dAmaCVHp_Xi",
        "outputId": "be45f8a4-caf8-4a52-bed7-ce6c4a691014"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existem 198738 exemplos negativos e 78786 exemplos positivos.\n",
            "Vamos usar apenas uma amostragem igualit√°ria de cada classe!\n",
            "Ent√£o nosso dataset final deve ter 78786 exemplos de cada classe.\n",
            "Dataset final: \n",
            "  - N classe negativa: 78786\n",
            "  - N classe positiva: 78786\n"
          ]
        }
      ],
      "source": [
        "nClass0 = len(class0)\n",
        "nClass1 = len(class1)\n",
        "\n",
        "print(f\"Existem {nClass0} exemplos negativos e {nClass1} exemplos positivos.\")\n",
        "print(f\"Vamos usar apenas uma amostragem igualit√°ria de cada classe!\")\n",
        "print(f\"Ent√£o nosso dataset final deve ter {nClass1} exemplos de cada classe.\")\n",
        "\n",
        "sampled_class0 = random.sample(class0, nClass1)\n",
        "sampled_class1 = random.sample(class1, nClass1)\n",
        "\n",
        "#sampled_class0 = random.sample(class0, 10000)\n",
        "#sampled_class1 = random.sample(class1, 10000)\n",
        "\n",
        "print(f\"Dataset final: \")\n",
        "print(f\"  - N classe negativa: {len(sampled_class0)}\")\n",
        "print(f\"  - N classe positiva: {len(sampled_class1)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5UFdVdfqR9g"
      },
      "source": [
        "Cria um array de tuplas para cada classe, sendo que a tupla est√° no formato (imagem, label), sendo:\n",
        "\n",
        "- imagem: a imagem no formato de matriz de pixels\n",
        "- label: int indicando o label (0 ou 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNkco2MLqUa8"
      },
      "outputs": [],
      "source": [
        "from matplotlib.image import imread\n",
        "import cv2\n",
        "\n",
        "def get_image_arrays(data, label):\n",
        "  img_arrays = []\n",
        "  for i in data:\n",
        "    if i.endswith(\".png\"):\n",
        "      img = cv2.imread(i, cv2.IMREAD_COLOR)\n",
        "      img_sized = cv2.resize(img, (50, 50), interpolation=cv2.INTER_LINEAR)\n",
        "      img_arrays.append([img_sized, label])\n",
        "  return img_arrays \n",
        "\n",
        "class0_array = get_image_arrays(sampled_class0, 0)\n",
        "class1_array = get_image_arrays(sampled_class1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tzWnoeSqX1O"
      },
      "source": [
        "Junte os dados numa coisa s√≥, e misture, mesclando as imagens para trazer aleatoriedade na sele√ß√£o. (Eu particulamente tenho medo disso, mas se voc√™ ta instruindo, eu confio) kkk "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvKhVUy5qarp",
        "outputId": "b9f3dfe4-a2b2-4023-97b6-e69a3a24e546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ],
      "source": [
        "all_data = np.concatenate((class0_array, class1_array))\n",
        "random.seed(42)\n",
        "random.shuffle(all_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IisCgsH5qtHL"
      },
      "source": [
        "Separe os dados (pixels), dos labels (ints). Devemos terminar com 157572 imagens, escaladas em 50x50 pixels, sendo que cada imagem √© expressa em 3 canais (RBG)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qr5X7voyquNB",
        "outputId": "811093b2-57cf-4e6b-f1b5-cd83901edac5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(157572, 50, 50, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ],
      "source": [
        "X = []\n",
        "y = []\n",
        "for image, label in all_data:\n",
        "  X.append(image)\n",
        "  y.append(label)\n",
        "\n",
        "X = np.array(X).reshape(-1, 50, 50, 3)\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnxgGUMIq5bi"
      },
      "source": [
        "Dividindo entre treino e teste: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gje1aXBwrEUe",
        "outputId": "88729ef6-bd58-479b-a923-d66efd6817ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(118179, 50, 50, 3) (39393, 50, 50, 3) (118179, 2) (39393, 2)\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCbGP5dPuy-E"
      },
      "source": [
        "Define a arquitetura da CNN -> camada de extra√ß√£o de features: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux7GZsXku66W"
      },
      "source": [
        "Define a arquitetura da CNN -> camada de decis√£o (classifica√ß√£o):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_NJGp7Vu-OY"
      },
      "outputs": [],
      "source": [
        "hand_model = Sequential()\n",
        "hand_model.add( Rescaling(1./255, input_shape=(50, 50, 3)) )\n",
        "\n",
        "hand_model.add( layers.Conv2D(16, kernel_size=10, activation=\"relu\") )\n",
        "hand_model.add( layers.MaxPooling2D(3) ) \n",
        "\n",
        "hand_model.add( layers.Conv2D(32, kernel_size=8, activation=\"relu\") )\n",
        "hand_model.add( layers.MaxPooling2D(2) ) \n",
        "\n",
        "hand_model.add( layers.Conv2D(32, kernel_size=2, activation=\"relu\") )\n",
        "hand_model.add( layers.MaxPooling2D(2) ) \n",
        "hand_model.add( layers.Flatten() )\n",
        "hand_model.add( layers.Dense(50, activation=\"relu\") )\n",
        "hand_model.add( layers.Dense(20, activation=\"relu\") )\n",
        "hand_model.add( layers.Dense(2, activation=\"softmax\") )\n",
        "\n",
        "#Compila o modelo:\n",
        "\n",
        "hand_model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "#Define a estrat√©gia de Early Stopping:\n",
        "\n",
        "es = EarlyStopping(monitor=\"val_accuracy\", \n",
        "                   patience=5,\n",
        "                   mode=\"max\",    #Se der erro comentar essa linha completa\n",
        "                   restore_best_weights=True)\n",
        "\n",
        "# Treinamento do modelo \n",
        "with tf.device(\"/device:GPU:0\"):\n",
        "  hand_model.fit(X_train, y_train,\n",
        "                 epochs=50,\n",
        "                 validation_split=0.2,\n",
        "                 batch_size=32,\n",
        "                 callbacks=[es])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qnY4mZsvrHi"
      },
      "source": [
        "Avalia√ß√£o do modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrxKcmP2vo7D",
        "outputId": "b9f9d72e-82f7-4716-8c9f-671c7734894c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3694/3694 [==============================] - 12s 3ms/step - loss: 0.2563 - accuracy: 0.8992\n",
            "A loss do modelo √© 0.26 e a ACC √©: 0.90\n"
          ]
        }
      ],
      "source": [
        "loss, acc = hand_model.evaluate(X_train, y_train)\n",
        "\n",
        "print(f\"A loss do modelo √© {loss:.2f} e a ACC √©: {acc:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHCLuwLb3Jhn"
      },
      "source": [
        "Plotando uma matriz de confus√£o com o resultado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDL6Hzdnvsc3"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(X_train, y_train):\n",
        "  labels=list(map(np.argmax,y_train))\n",
        "  labels_pred = list(map(np.argmax, y_train))\n",
        "\n",
        "  cf_matrix = confusion_matrix(labels, labels_pred)\n",
        "  sns.heatmap(cf_matrix, annot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "mOlZDhb_v7IE",
        "outputId": "8826d301-fbbd-482f-8edc-548013c51826"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "predicoes = hand_model.predict(X_train)\n",
        "plot_confusion_matrix(y_train, predicoes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huxvLmYxcl_o"
      },
      "source": [
        "## Trabalhando com Transfer Learning do modelo VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5QTN2EVcoi1"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from tensorflow.keras.applications.vgg16 import VGG16 # Arquitetura padr√£o VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input # Fun√ß√£o pra processar as imagens sobre a vis√£o da arquitetura VGG16\n",
        "\n",
        "# Preprocessa os dados de treinamento e teste\n",
        "X_train = preprocess_input(X_train)\n",
        "X_test = preprocess_input(X_test)\n",
        "\n",
        "# Importa o modelo de base, que ser√° a VGG16 ***sem*** a camada de decis√£o\n",
        "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=X_train[0].shape)\n",
        "\n",
        "# Usar os mesmos pesos da rede treinada (sem fine tunning)\n",
        "base_model.trainable = False\n",
        "#base_model.trainable = True\n",
        "\n",
        "# Define as camadas treinaveis (com fine tunning)\n",
        "#print(\"N√∫mero de camadas na base modelo: \", len(base_model.layers)) # Printa o n√∫mero de camadas\n",
        "\n",
        "#fine_tune_at = 100\n",
        "#for layer in base_model.layers[:fine_tune_at]:\n",
        "# layer.trainable = False # At√© a camada 100, n√£o √© treinavel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGRCWI_KctLg"
      },
      "outputs": [],
      "source": [
        "# Observa a estrutura do modelo importado\n",
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dV1OdiQcwR-"
      },
      "source": [
        "## Adicionar a camada de decis√£o (classifica√ß√£o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgK7mHqVcw5l"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define as camadas\n",
        "flatten_layer = layers.Flatten()\n",
        "dense_layer_1 = layers.Dense(50, activation=\"relu\")\n",
        "dense_layer_2 = layers.Dense(20, activation=\"relu\")\n",
        "prediction_layer = layers.Dense(2, activation=\"softmax\")\n",
        "\n",
        "# Estabelece o pipeline completo\n",
        "model = models.Sequential([\n",
        "    base_model,       # A base CNN da VGG16\n",
        "    flatten_layer,    # Achata as features que saem da VGG16 (base_model) \n",
        "    dense_layer_1,    # Primeira camada oculta fully connected\n",
        "    dense_layer_2,    # Segunda camada oculta fully connected\n",
        "    prediction_layer  # Camada de sa√≠da, com 5 neur√¥nios para classifica√ß√£o\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umgJteXKc0tN"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y3R-HDnc2NG"
      },
      "source": [
        "## Treinamento do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qs9uInlec3cM"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Compila o novo modelo\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Define a estrat√©gia de Early Stopping\n",
        "es = EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=5, restore_best_weights=True)\n",
        "\n",
        "# Treina o modelo\n",
        "model.fit(X_train, y_train, epochs=50, validation_split=0.2, batch_size=32, callbacks=[es])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6dnje3bc4zZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9aa1149-e62c-43b7-c8ea-101a974d3bc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1232/1232 [==============================] - 13s 11ms/step - loss: 0.3271 - accuracy: 0.9026\n",
            "A loss do modelo √© 0.33 e a ACC √©: 0.90\n"
          ]
        }
      ],
      "source": [
        "# Avalia√ß√£o do modelo\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "\n",
        "print(f\"A loss do modelo √© {loss:.2f} e a ACC √©: {acc:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGrPZZLjc6Dt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "8353d2fc-ea93-4209-a4c7-dae60baaf2ea"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "preds = model.predict(X_test)\n",
        "plot_confusion_matrix(y_test, preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Considera√ß√µes finais! ‚òù Que rufem os tambores.... ü™ó (s√≥ encontrei o emoji da sanfona) kk\n",
        "\n"
      ],
      "metadata": {
        "id": "2XF6WJbOQv-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A 1¬∫ proposta foi para criar uma rede convulacional do 0, que resultou em uma Acur√°cia de 0.90 e a Loss do modelo foi de 0.26. (UAU)\n",
        "\n",
        "### E a 2¬∫ que foi utilizar a Transfer Learning do modelo VGG16, que resultou em uma Acur√°cia de 0.90 e a Loss do modelo foi de 0.33. (UAU again).\n",
        "\n",
        "### Ou seja tanto a rede treinada do 0, quanto a transfer learning foram eficazes para a an√°lise das imagens. ‚ú®"
      ],
      "metadata": {
        "id": "D1u0zgUDROsm"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}